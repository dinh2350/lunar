# Day 3 â€” Tokens, Temperature, and Configuration

> ğŸ¯ **DAY GOAL:** Understand how LLMs process text (tokens) and control AI creativity (temperature)

---

## ğŸ“š CONCEPT 1: Tokens

### WHAT â€” Simple Definition

**Token = the smallest unit that an LLM reads and writes.** A token is usually a word or piece of a word.

LLMs don't see text like you do. They see **numbers** (tokens):

```
You see:  "Hello, how are you?"
LLM sees: [15496, 11, 1268, 527, 499, 30]
           Hello   ,   how  are you   ?
```

### WHY â€” Why Do Tokens Matter?

**Three critical reasons:**

1. **Context window limit:** LLMs can only process a fixed number of tokens at once.
   ```
   llama3.3  â†’ 128,000 tokens  (~100 pages of text)
   qwen2.5   â†’ 32,000 tokens   (~25 pages)
   
   If your conversation exceeds this â†’ the AI forgets old messages!
   ```

2. **Cost (for paid APIs):** You pay per token.
   ```
   GPT-4o: $2.50 per 1 million input tokens
   1,000 tokens â‰ˆ 750 words
   
   With Ollama: $0 (free!) â€” but you still need to watch context limits
   ```

3. **Speed:** More tokens = slower response.
   ```
   Short prompt (50 tokens)   â†’ fast response (~1 second)
   Long prompt (5000 tokens)  â†’ slower response (~5-10 seconds)
   ```

### WHEN â€” When Do You Need to Think About Tokens?

- When conversations get very long (approaching context window)
- When using RAG (need to fit retrieved text + question + history in one request)
- When optimizing speed (shorter prompts = faster)
- When calculating costs (paid APIs)

### HOW â€” How Tokenization Works

**Rule of thumb:** 1 token â‰ˆ Â¾ of a word (or about 4 characters in English)

```
"Hello"        â†’ 1 token   [Hello]
"Hello world"  â†’ 2 tokens  [Hello][ world]
"TypeScript"   â†’ 2 tokens  [Type][Script]    â† long word = multiple tokens
"AI"           â†’ 1 token   [AI]
"Xin chÃ o"     â†’ 3 tokens  [X][in][ chÃ o]   â† Vietnamese uses more tokens
"ğŸ‰"           â†’ 1-3 tokens                   â† emojis use extra tokens!
```

**Visual: How context window fills up**
```
Context Window = 128,000 tokens
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ [system prompt ~~200 tokens~~]                            â”‚
â”‚ [user message 1 ~~50 tokens~~]                            â”‚
â”‚ [assistant reply 1 ~~100 tokens~~]                        â”‚
â”‚ [user message 2 ~~50 tokens~~]                            â”‚
â”‚ [assistant reply 2 ~~150 tokens~~]                        â”‚
â”‚ ... (conversation history grows) ...                     â”‚
â”‚                                                          â”‚
â”‚               ~~127,000 tokens remaining~~                â”‚
â”‚                                                          â”‚
â”‚ If full â†’ oldest messages must be removed!               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ğŸ”— NODE.JS ANALOGY

Tokens are like **request body size limits** in Express:

```typescript
// Express has a body size limit:
app.use(express.json({ limit: '1mb' }));  // max ~1MB per request

// LLMs have a token limit:
ollama.chat({ model: 'llama3.3' });  // max 128,000 tokens per request

// Both: if you exceed the limit â†’ error or data gets cut off
```

---

## ğŸ“š CONCEPT 2: Temperature

### WHAT â€” Simple Definition

**Temperature = a number from 0 to 2 that controls how "creative" the AI is.**

```
Temperature 0   â†’ Always picks the MOST likely next word (deterministic)
Temperature 0.7 â†’ Usually picks likely words, sometimes surprises (balanced)
Temperature 1   â†’ Spreads probability more evenly (creative/random)
Temperature 2   â†’ Almost random (chaotic, often nonsensical)
```

### WHY â€” Why Does Temperature Exist?

Because different tasks need different levels of creativity:

| Task | Best Temperature | Why |
|---|---|---|
| Code generation | 0 - 0.2 | Code must be correct, not creative |
| Factual Q&A | 0 - 0.3 | "What is 2+2?" should always be 4 |
| Chat assistant | 0.5 - 0.7 | Friendly but reliable |
| Creative writing | 0.8 - 1.0 | Need variety and surprise |
| Brainstorming | 1.0 - 1.2 | Want unusual/wild ideas |

### WHEN â€” When to Use What Temperature?

```
YOUR LUNAR AGENT:
â”œâ”€â”€ Answering factual questions â†’ temperature 0.3
â”œâ”€â”€ General conversation        â†’ temperature 0.7 (default)
â”œâ”€â”€ Creative tasks              â†’ temperature 1.0
â””â”€â”€ Code writing                â†’ temperature 0.1
```

### HOW â€” How Temperature Works (Intuitively)

When the AI predicts the next word, it calculates a probability for EVERY possible word:

```
Input: "The capital of France is ___"

Temperature = 0 (no randomness):
  "Paris"     â†’ 95%  â† ALWAYS picks this
  "a"         â†’ 2%
  "the"       â†’ 1%
  "Lyon"      â†’ 0.5%
  
Temperature = 0.7 (some randomness):
  "Paris"     â†’ 78%  â† Usually picks this
  "a"         â†’ 8%
  "the"       â†’ 5%   â† Sometimes picks this
  "Lyon"      â†’ 3%
  
Temperature = 1.5 (very random):
  "Paris"     â†’ 35%
  "a"         â†’ 20%
  "the"       â†’ 15%
  "Lyon"      â†’ 10%  â† Might pick this!
  "beautiful" â†’ 8%   â† Or even this!
```

**Visual: Temperature flattens the probability curve**
```
Temperature = 0 (sharp peak â€” deterministic)
  ^
  â”‚  â–ˆ
  â”‚  â–ˆ
  â”‚  â–ˆ
  â”‚  â–ˆâ–‘â–‘â–‘â–‘
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ words

Temperature = 0.7 (softer peak â€” balanced)
  ^
  â”‚  â–“
  â”‚  â–“â–’
  â”‚  â–“â–’â–‘
  â”‚  â–“â–’â–‘â–‘â–‘
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ words

Temperature = 1.5 (flat â€” random)
  ^
  â”‚
  â”‚  â–‘â–‘â–‘â–‘â–‘â–‘
  â”‚  â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘
  â”‚  â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ words
```

### ğŸ”— NODE.JS ANALOGY

Temperature is like choosing between a **strict linter** and **no linter**:

```typescript
// Temperature 0 = eslint with strict rules
// â†’ Always does the same thing, no variation, very predictable

// Temperature 0.7 = eslint with relaxed rules
// â†’ Mostly follows patterns but allows some flexibility

// Temperature 1.5 = no linter at all
// â†’ Complete freedom, might be creative, might be nonsense
```

---

## ğŸ“š CONCEPT 3: Other Important LLM Settings

### top_p (Nucleus Sampling)

**WHAT:** Another way to control randomness. Instead of adjusting ALL probabilities (temperature), it only considers the TOP words that add up to a certain probability.

```
top_p = 0.9 â†’ only consider words that make up 90% of the probability
  "Paris" (95%) â†’ YES (within top 90%)
  "a" (2%)      â†’ NO  (we already have >90%)

top_p = 0.5 â†’ only consider words that make up 50%
  "Paris" (95%) â†’ YES (it alone exceeds 50%, so only "Paris" is considered)
```

**WHEN to use:** Usually you use EITHER temperature OR top_p, not both. Temperature is simpler to understand.

### max_tokens (Response Length Limit)

**WHAT:** Maximum number of tokens the AI can output in its response.

```
max_tokens: 100   â†’ short answers only (~75 words)
max_tokens: 2048  â†’ medium answers (~1500 words)  
max_tokens: 8192  â†’ very long answers (~6000 words)
```

**WHY:** Without a limit, the AI might generate a 10,000-word essay when you wanted a one-line answer. Also prevents the AI from running forever.

### frequency_penalty

**WHAT:** Discourages the AI from repeating the same words.

```
frequency_penalty: 0   â†’ AI may repeat itself
frequency_penalty: 0.5 â†’ AI avoids using the same word twice
frequency_penalty: 2   â†’ AI strongly avoids repetition (may start using weird synonyms)
```

---

## ğŸ”¨ HANDS-ON: Build a Configurable LLM Client

### Step 1: Add Configuration Interface (15 minutes)

Update `packages/agent/src/llm/client.ts`:

```typescript
import { Ollama } from 'ollama';

// === CONNECTION ===
export const ollama = new Ollama({
  host: 'http://localhost:11434',
});

// === CONFIGURATION ===

/**
 * LLM Configuration
 * Controls how the AI behaves for each request.
 * 
 * Think of this like Express request options:
 *   { timeout: 5000, maxRetries: 3 }
 * But for AI:
 *   { temperature: 0.7, maxTokens: 2048 }
 */
export interface LLMConfig {
  model: string;          // which AI brain to use
  temperature: number;    // 0 = deterministic, 1 = creative
  maxTokens: number;      // max response length in tokens
}

/** Default config â€” good for general conversation */
const DEFAULT_CONFIG: LLMConfig = {
  model: 'llama3.3',
  temperature: 0.7,
  maxTokens: 2048,
};

// === PRESET CONFIGS FOR DIFFERENT TASKS ===

/** For code generation â€” precise, no creativity */
export const CODE_CONFIG: Partial<LLMConfig> = {
  temperature: 0.1,
  maxTokens: 4096,
};

/** For factual Q&A â€” deterministic, consistent answers */
export const FACTUAL_CONFIG: Partial<LLMConfig> = {
  temperature: 0,
  maxTokens: 1024,
};

/** For creative tasks â€” more variation */
export const CREATIVE_CONFIG: Partial<LLMConfig> = {
  temperature: 1.0,
  maxTokens: 4096,
};

// === MAIN CHAT FUNCTION ===

/**
 * Send a message to the AI.
 * 
 * @param userMessage - What the user typed
 * @param systemPrompt - Instructions for the AI (optional)
 * @param config - Override default settings (optional)
 * @returns The AI's response text
 */
export async function chat(
  userMessage: string,
  systemPrompt?: string,
  config: Partial<LLMConfig> = {}
): Promise<string> {
  // Merge: defaults â† overrides
  const cfg = { ...DEFAULT_CONFIG, ...config };

  const response = await ollama.chat({
    model: cfg.model,
    messages: [
      {
        role: 'system',
        content: systemPrompt ?? 'You are Lunar, a helpful personal assistant. Be concise.',
      },
      {
        role: 'user',
        content: userMessage,
      },
    ],
    options: {
      temperature: cfg.temperature,
      num_predict: cfg.maxTokens,  // Ollama uses "num_predict" instead of "max_tokens"
    },
  });

  return response.message.content;
}
```

### Step 2: Update CLI with Slash Commands (20 minutes)

Update `packages/agent/src/cli.ts`:

```typescript
import * as readline from 'readline';
import { chat, ollama, CODE_CONFIG, CREATIVE_CONFIG, FACTUAL_CONFIG } from './llm/client.js';
import type { LLMConfig } from './llm/client.js';

const rl = readline.createInterface({
  input: process.stdin,
  output: process.stdout,
});

// Current configuration (can be changed at runtime)
let currentConfig: Partial<LLMConfig> = {};
let currentSystemPrompt = 'You are Lunar, a helpful personal assistant. Be concise.';

/**
 * Handle slash commands like /temp, /model, /preset
 * Returns true if the input was a command, false if it's a regular message
 */
function handleCommand(input: string): boolean {
  const parts = input.split(' ');
  const command = parts[0].toLowerCase();

  switch (command) {
    case '/temp':
    case '/temperature': {
      const temp = parseFloat(parts[1]);
      if (isNaN(temp) || temp < 0 || temp > 2) {
        console.log('Usage: /temp <0-2>  (e.g., /temp 0.7)\n');
        return true;
      }
      currentConfig.temperature = temp;
      console.log(`âœ… Temperature set to ${temp}\n`);
      return true;
    }

    case '/model': {
      const model = parts[1];
      if (!model) {
        console.log('Usage: /model <name>  (e.g., /model qwen2.5:7b)\n');
        return true;
      }
      currentConfig.model = model;
      console.log(`âœ… Model switched to ${model}\n`);
      return true;
    }

    case '/preset': {
      const preset = parts[1]?.toLowerCase();
      switch (preset) {
        case 'code':
          currentConfig = { ...CODE_CONFIG };
          console.log('âœ… Preset: CODE (temp=0.1, precise)\n');
          break;
        case 'creative':
          currentConfig = { ...CREATIVE_CONFIG };
          console.log('âœ… Preset: CREATIVE (temp=1.0, varied)\n');
          break;
        case 'factual':
          currentConfig = { ...FACTUAL_CONFIG };
          console.log('âœ… Preset: FACTUAL (temp=0, deterministic)\n');
          break;
        default:
          console.log('Usage: /preset <code|creative|factual>\n');
      }
      return true;
    }

    case '/system': {
      const newPrompt = parts.slice(1).join(' ');
      if (!newPrompt) {
        console.log(`Current system prompt: "${currentSystemPrompt}"\n`);
        return true;
      }
      currentSystemPrompt = newPrompt;
      console.log(`âœ… System prompt updated\n`);
      return true;
    }

    case '/config': {
      console.log('Current config:', { ...{ model: 'llama3.3', temperature: 0.7, maxTokens: 2048 }, ...currentConfig });
      console.log(`System prompt: "${currentSystemPrompt}"\n`);
      return true;
    }

    case '/help': {
      console.log(`
Available commands:
  /temp <0-2>                Set temperature (0=precise, 1=creative)
  /model <name>              Switch AI model (e.g., qwen2.5:7b)
  /preset <code|creative|factual>  Use preconfigured settings
  /system <prompt>           Change system prompt
  /config                    Show current settings
  /help                      Show this help
  exit                       Quit
      `);
      return true;
    }

    default:
      return false; // not a command â€” treat as regular message
  }
}

function askQuestion(): void {
  rl.question('You: ', async (input: string) => {
    if (input.toLowerCase() === 'exit' || input.toLowerCase() === 'quit') {
      console.log('Goodbye! ğŸ‘‹');
      rl.close();
      return;
    }

    if (!input.trim()) { askQuestion(); return; }

    // Check if it's a command
    if (input.startsWith('/')) {
      handleCommand(input);
      askQuestion();
      return;
    }

    try {
      console.log('Lunar is thinking...');
      const answer = await chat(input, currentSystemPrompt, currentConfig);
      console.log(`\nLunar: ${answer}\n`);
    } catch (error: any) {
      console.error(`\nâŒ Error: ${error.message}\n`);
    }

    askQuestion();
  });
}

// Start
console.log('â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—');
console.log('â•‘  ğŸŒ™ Lunar AI â€” Personal Assistant  â•‘');
console.log('â•‘  Type /help for commands           â•‘');
console.log('â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n');

askQuestion();
```

### Step 3: Experiment with Temperature (15 minutes)

Run the chatbot and try these experiments:

```
=== EXPERIMENT 1: Temperature 0 (Deterministic) ===
/temp 0
You: Give me a random number between 1 and 100
Lunar: 42
You: Give me a random number between 1 and 100
Lunar: 42                        â† SAME ANSWER!
You: Give me a random number between 1 and 100
Lunar: 42                        â† ALWAYS THE SAME!

=== EXPERIMENT 2: Temperature 1 (Creative) ===
/temp 1
You: Give me a random number between 1 and 100
Lunar: 73
You: Give me a random number between 1 and 100
Lunar: 28                        â† DIFFERENT!
You: Give me a random number between 1 and 100
Lunar: 91                        â† DIFFERENT AGAIN!

=== EXPERIMENT 3: Presets ===
/preset code
You: Write a function to reverse a string
Lunar: [precise, correct code]

/preset creative
You: Write a poem about TypeScript
Lunar: [creative, varied poem]

=== EXPERIMENT 4: Different System Prompts ===
/system You are a pirate. Always speak like a pirate.
You: What is JavaScript?
Lunar: Yarr! JavaScript be the language of the seven seas of the web!

/system You always respond in JSON format.
You: What is Node.js?
Lunar: {"answer": "Node.js is a JavaScript runtime", "creator": "Ryan Dahl"}
```

---

## âœ… CHECKLIST â€” Verify Before Moving to Day 4

- [ ] Your `chat()` function accepts `config` parameter
- [ ] `/temp 0` makes answers deterministic (same answer every time)
- [ ] `/temp 1` makes answers varied (different each time)
- [ ] `/preset code|creative|factual` switches presets
- [ ] `/system` changes the system prompt at runtime
- [ ] You can explain: "A token is ___, temperature controls ___"

---

## ğŸ’¡ KEY TAKEAWAY

**Tokens are how LLMs see text (words â†’ numbers). Temperature controls creativity (0 = precise, 1 = creative).** These two concepts explain most LLM behavior. When the AI gives wrong answers â†’ lower temperature. When it's too boring â†’ raise it.

---

## â“ SELF-CHECK QUESTIONS

1. **How many tokens is the word "TypeScript"?**
   <details><summary>Answer</summary>Usually 2 tokens: [Type][Script]. Long words get split into sub-words.</details>

2. **Your context window is 128,000 tokens. About how many pages of text is that?**
   <details><summary>Answer</summary>About 100 pages. (1 page â‰ˆ 250 words â‰ˆ 333 tokens â†’ 128,000 / 333 â‰ˆ 384 pages... but with conversation overhead, roughly 100 usable pages.)</details>

3. **You want the AI to write code. What temperature should you use and why?**
   <details><summary>Answer</summary>0 to 0.2. Code must be correct â€” you want the most likely (correct) token every time. Creativity in code = bugs.</details>

4. **At temperature 0, if you ask the same question 10 times, what happens?**
   <details><summary>Answer</summary>You get the same answer all 10 times (deterministic). Temperature 0 always picks the highest-probability token.</details>

5. **What is `num_predict` in Ollama?**
   <details><summary>Answer</summary>Same as `max_tokens` â€” the maximum number of tokens the AI can output in its response. Ollama uses a different parameter name.</details>

6. **Why might Vietnamese text use more tokens than English?**
   <details><summary>Answer</summary>Most LLMs are trained primarily on English text, so English words are efficiently encoded as single tokens. Vietnamese words might be split into more sub-word tokens because they're less common in training data.</details>

---

**Next â†’ [Day 4: Conversation History + System Prompts](day-04.md)**
