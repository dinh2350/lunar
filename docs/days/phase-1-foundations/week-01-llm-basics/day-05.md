# Day 5 â€” Streaming + Multiple Models

> ğŸ¯ **DAY GOAL:** Make AI responses appear word-by-word (streaming) and learn to switch between models

---

## ğŸ“š CONCEPT 1: Streaming Responses

### WHAT â€” Simple Definition

**Streaming = getting the AI's response word-by-word as it's generated**, instead of waiting for the entire response to finish.

```
WITHOUT streaming (bad UX):
  User types question â†’ waits 5 seconds seeing nothing â†’ entire answer appears at once
  [__________________...waiting...__________________] [FULL ANSWER APPEARS]

WITH streaming (good UX):
  User types question â†’ words appear immediately, one by one
  [The] [capital] [of] [Vietnam] [is] [Hanoi] [.]
  â†‘ first word appears in <0.5 seconds!
```

### WHY â€” Why Does Streaming Matter?

**Perceived speed.** The AI doesn't actually respond faster â€” streaming just shows you the output AS it's being generated.

```
Without streaming:
  Time-to-first-word: 5 seconds (user stares at blank screen)
  Total time: 5 seconds
  User thinks: "this is slow"

With streaming:
  Time-to-first-word: 0.3 seconds (user sees first word immediately)
  Total time: 5 seconds (same!)
  User thinks: "this is fast!"
```

**The same response takes the same total time. But streaming FEELS 10x faster.**

Every production AI app uses streaming: ChatGPT, Claude, Gemini â€” they all stream.

### WHEN â€” When to Use Streaming?

- âœ… **Chat interfaces** â€” always stream (UX is dramatically better)
- âœ… **CLI tools** â€” stream for better experience
- âœ… **Any response > 1 second** â€” stream it
- âŒ **Backend API calls where you need the full text** (e.g., evaluating a response) â€” use non-streaming (simpler code)

### HOW â€” How Does Streaming Work?

```
WITHOUT streaming:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Your Codeâ”‚  â”€â”€â”€â”€â”€â”€â–ºâ”‚ Ollama   â”‚  (AI generates all tokens internally)
â”‚          â”‚  â—„â”€â”€â”€â”€â”€â”€â”‚          â”‚  Returns: "The capital of Vietnam is Hanoi."
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  1 big  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              response

WITH streaming:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Your Codeâ”‚  â”€â”€â”€â”€â”€â”€â–ºâ”‚ Ollama   â”‚
â”‚          â”‚  â—„â”€â”€â”€â”€â”€â”€â”‚          â”‚  Chunk 1: "The"
â”‚          â”‚  â—„â”€â”€â”€â”€â”€â”€â”‚          â”‚  Chunk 2: " capital"
â”‚          â”‚  â—„â”€â”€â”€â”€â”€â”€â”‚          â”‚  Chunk 3: " of"
â”‚          â”‚  â—„â”€â”€â”€â”€â”€â”€â”‚          â”‚  Chunk 4: " Vietnam"
â”‚          â”‚  â—„â”€â”€â”€â”€â”€â”€â”‚          â”‚  Chunk 5: " is"
â”‚          â”‚  â—„â”€â”€â”€â”€â”€â”€â”‚          â”‚  Chunk 6: " Hanoi"
â”‚          â”‚  â—„â”€â”€â”€â”€â”€â”€â”‚          â”‚  Chunk 7: "."
â”‚          â”‚  â—„â”€â”€â”€â”€â”€â”€â”‚          â”‚  [DONE]
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  many   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              small chunks
```

**Under the hood:** Streaming uses **Server-Sent Events (SSE)** or **chunked HTTP responses** â€” the server keeps the connection open and sends data as it's produced.

### ğŸ”— NODE.JS ANALOGY

Streaming is like the difference between `readFile` and `createReadStream`:

```typescript
// NON-STREAMING: Wait for entire file to load into memory
const data = await fs.readFile('big-file.txt', 'utf8');
console.log(data);
// â†‘ waits until ENTIRE file is read, then prints all at once

// STREAMING: Process file chunk by chunk
const stream = fs.createReadStream('big-file.txt', 'utf8');
stream.on('data', (chunk) => {
  process.stdout.write(chunk);  // prints as each chunk arrives
});
// â†‘ prints immediately as data arrives â€” same total time, better UX

// LLM streaming is the EXACT same pattern:
const stream = await ollama.chat({ model: 'llama3.3', messages, stream: true });
for await (const chunk of stream) {
  process.stdout.write(chunk.message.content);  // prints each token
}
```

You already know `ReadableStream`, `EventEmitter`, `for await`. LLM streaming uses the same concepts!

---

## ğŸ“š CONCEPT 2: Multiple Models (Choosing the Right Brain)

### WHAT â€” Simple Definition

**Different AI models have different strengths**, just like different databases are good at different things.

```
Models are like databases:
  PostgreSQL = reliable, good at everything â†’ llama3.3
  Redis = super fast, limited features     â†’ qwen2.5:3b
  MongoDB = flexible, good for documents   â†’ gemma2:27b
```

### WHY â€” Why Use Multiple Models?

| Situation | Best Choice | Why |
|---|---|---|
| General conversation | llama3.3 (8B) | Best balance of quality and speed |
| Fast simple answers | qwen2.5:3b | Small, fast, good enough for simple tasks |
| Need highest quality | llama3.3 (70B) or cloud API | Best reasoning, but slow/expensive |
| Embeddings (search) | nomic-embed-text | Designed specifically for embeddings |
| Code generation | codestral or deepseek-coder | Trained specifically on code |

### WHEN â€” When to Switch Models?

**Lunar's fallback strategy** (from the architecture doc):

```
Request comes in
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Try Ollama (local) â”‚ â† Free, private, no internet needed
â”‚ llama3.3        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ If fails (too slow, error, model not loaded)
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Try Groq (cloud) â”‚ â† Free tier, VERY fast (300 tokens/sec)
â”‚ llama3.3-70b    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ If fails (rate limit, network error)
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Try Gemini (cloud)â”‚ â† Free tier, good quality
â”‚ gemini-1.5-flash â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ If all fail
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Return error     â”‚
â”‚ "AI unavailable" â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### HOW â€” How Model Sizes Work

Model names often include a size: `llama3.3:8b`, `qwen2.5:7b`, `llama3.3:70b`

```
"b" = billions of parameters (the "brain cells" of the model)

1b-3b   = Small   â†’ fast, basic tasks, fits in 2GB RAM
7b-8b   = Medium  â†’ good balance, fits in 4-6GB RAM â† SWEET SPOT for local
13b     = Large   â†’ better quality, needs 8GB+ RAM
30b-70b = XL      â†’ near-GPT-4 quality, needs 40GB+ RAM or cloud
```

**Rule of thumb for your Mac:**
```
8GB RAM Mac  â†’ use 3b-8b models
16GB RAM Mac â†’ use 8b-13b models
32GB+ RAM    â†’ use up to 30b models
```

### ğŸ”— NODE.JS ANALOGY

Multiple models = choosing the right npm package for the job:

```typescript
// Like choosing between lightweight vs full-featured:
// express (small, fast)      â†” qwen2.5:3b (small, fast)
// fastify (balanced)         â†” llama3.3:8b (balanced)
// nest.js (feature-rich)     â†” llama3.3:70b (high quality)

// And using fallbacks:
// Like trying Redis cache first, then PostgreSQL:
try {
  return await redis.get(key);      // fast, might miss
} catch {
  return await postgres.query(sql); // slower, always works
}

// Same pattern for LLMs:
try {
  return await ollama.chat(...);     // local, free, might be slow
} catch {
  return await groq.chat(...);       // cloud, free tier, always fast
}
```

---

## ğŸ“š CONCEPT 3: The LLM Provider Interface (Clean Architecture)

### WHAT â€” Simple Definition

**An interface that makes all LLM providers look the same to your code.** Whether you use Ollama, Groq, or Gemini, the calling code is identical.

### WHY â€” Why Use an Interface?

Without an interface, switching models means changing code everywhere:

```typescript
// BAD: tightly coupled to Ollama
if (provider === 'ollama') {
  response = await ollamaClient.chat({ model: 'llama3.3', messages });
} else if (provider === 'groq') {
  response = await groqClient.createChatCompletion({ model: 'llama3', messages });
} else if (provider === 'gemini') {
  response = await geminiClient.generateContent(messages.map(...));
}
// Nightmare when adding a new provider!
```

With an interface:

```typescript
// GOOD: all providers implement the same interface
const response = await provider.chat(messages);
// Don't care if it's Ollama, Groq, or Gemini â€” same code!
```

### ğŸ”— NODE.JS ANALOGY

Like how Mongoose and Prisma both give you `.find()` even though MongoDB and PostgreSQL are completely different:

```typescript
// Mongoose:
await User.find({ name: 'Hao' });

// Prisma:
await prisma.user.findMany({ where: { name: 'Hao' } });

// Different backends, similar interface â†’ easy to switch
```

---

## ğŸ”¨ HANDS-ON: Add Streaming + Provider Interface

### Step 1: Create the LLM Provider Interface (15 minutes)

Update `packages/agent/src/llm/types.ts`:

```typescript
/**
 * A single message in the conversation.
 */
export interface Message {
  role: 'system' | 'user' | 'assistant' | 'tool';
  content: string;
}

/**
 * LLM Provider Interface â€” the contract every LLM provider must follow.
 * 
 * WHY: So we can swap Ollama â†” Groq â†” Gemini without changing any calling code.
 * 
 * Think of it like a database driver interface:
 *   interface DB { query(sql: string): Promise<Row[]> }
 *   class PostgresDB implements DB { ... }
 *   class MySQLDB implements DB { ... }
 */
export interface LLMProvider {
  /** Provider name (for logging) */
  readonly name: string;

  /** Send messages, get full response */
  chat(messages: Message[], options?: ChatOptions): Promise<ChatResponse>;

  /** Send messages, get response token by token */
  chatStream(
    messages: Message[],
    onToken: (token: string) => void,
    options?: ChatOptions,
  ): Promise<ChatResponse>;
}

export interface ChatOptions {
  model?: string;
  temperature?: number;
  maxTokens?: number;
}

export interface ChatResponse {
  content: string;        // the full response text
  model: string;          // which model was actually used
  tokensUsed?: number;    // how many tokens (if available)
}
```

### Step 2: Implement the Ollama Provider (20 minutes)

Update `packages/agent/src/llm/client.ts`:

```typescript
import { Ollama } from 'ollama';
import type { LLMProvider, Message, ChatOptions, ChatResponse } from './types.js';

/**
 * Ollama LLM Provider â€” runs AI locally on your Mac.
 * 
 * WHAT: Wraps the Ollama API into a clean provider interface
 * WHY:  So we can swap to Groq/Gemini later without changing other code
 * WHEN: Default provider for development (free, private, offline)
 */
export class OllamaProvider implements LLMProvider {
  readonly name = 'ollama';
  private client: Ollama;
  private defaultModel: string;

  constructor(host = 'http://localhost:11434', defaultModel = 'llama3.3') {
    this.client = new Ollama({ host });
    this.defaultModel = defaultModel;
  }

  /**
   * Non-streaming chat â€” waits for full response.
   * Use when: you need the complete text before proceeding
   * (e.g., evaluating the response, parsing JSON from it)
   */
  async chat(messages: Message[], options?: ChatOptions): Promise<ChatResponse> {
    const model = options?.model ?? this.defaultModel;

    const response = await this.client.chat({
      model,
      messages,
      options: {
        temperature: options?.temperature ?? 0.7,
        num_predict: options?.maxTokens ?? 2048,
      },
    });

    return {
      content: response.message.content,
      model,
      tokensUsed: response.eval_count,
    };
  }

  /**
   * Streaming chat â€” calls onToken for each word as it's generated.
   * Use when: displaying to user (CLI, chat UI, Telegram)
   * 
   * How it works:
   *   1. Ollama starts generating tokens
   *   2. Each token is sent to onToken() immediately
   *   3. Meanwhile, we build the full string
   *   4. When done, return the complete response
   */
  async chatStream(
    messages: Message[],
    onToken: (token: string) => void,
    options?: ChatOptions,
  ): Promise<ChatResponse> {
    const model = options?.model ?? this.defaultModel;

    const stream = await this.client.chat({
      model,
      messages,
      stream: true,   // â† THE KEY: enables streaming
      options: {
        temperature: options?.temperature ?? 0.7,
        num_predict: options?.maxTokens ?? 2048,
      },
    });

    // Collect the full response while streaming each token
    let fullContent = '';

    // "for await" is the JS way to read async streams â€” you already know this!
    // Just like: for await (const chunk of readStream) { ... }
    for await (const chunk of stream) {
      const token = chunk.message.content;
      fullContent += token;
      onToken(token);  // â† print/display immediately
    }

    return {
      content: fullContent,
      model,
    };
  }
}

// Default provider instance
export const defaultProvider = new OllamaProvider();
```

### Step 3: Update CLI to Use Streaming (20 minutes)

Update `packages/agent/src/cli.ts`:

```typescript
import * as readline from 'readline';
import { OllamaProvider } from './llm/client.js';
import type { Message } from './llm/types.js';

const rl = readline.createInterface({
  input: process.stdin,
  output: process.stdout,
});

// The AI provider
const provider = new OllamaProvider();

// Conversation history
const messages: Message[] = [
  {
    role: 'system',
    content: 'You are Lunar, a helpful personal assistant. Be concise and friendly.',
  },
];

// Config
let temperature = 0.7;
let model = 'llama3.3';
let useStreaming = true;

/**
 * Chat with streaming â€” tokens appear word by word
 */
async function chatWithStreaming(userInput: string): Promise<string> {
  messages.push({ role: 'user', content: userInput });

  process.stdout.write('\nLunar: ');  // print prefix without newline

  const response = await provider.chatStream(
    messages,
    (token) => {
      process.stdout.write(token);  // â† each word appears immediately!
    },
    { model, temperature },
  );

  process.stdout.write('\n\n');  // add newline after response

  messages.push({ role: 'assistant', content: response.content });
  return response.content;
}

/**
 * Chat without streaming â€” full response at once
 */
async function chatWithoutStreaming(userInput: string): Promise<string> {
  messages.push({ role: 'user', content: userInput });

  console.log('Lunar is thinking...');
  const response = await provider.chat(messages, { model, temperature });

  console.log(`\nLunar: ${response.content}\n`);
  messages.push({ role: 'assistant', content: response.content });
  return response.content;
}

/**
 * Handle slash commands
 */
function handleCommand(input: string): boolean {
  const [cmd, ...args] = input.split(' ');

  switch (cmd) {
    case '/stream': {
      useStreaming = !useStreaming;
      console.log(`âœ… Streaming: ${useStreaming ? 'ON' : 'OFF'}\n`);
      return true;
    }
    case '/temp': {
      const t = parseFloat(args[0]);
      if (!isNaN(t)) { temperature = t; console.log(`âœ… Temperature: ${t}\n`); }
      return true;
    }
    case '/model': {
      if (args[0]) { model = args[0]; console.log(`âœ… Model: ${model}\n`); }
      else console.log(`Current model: ${model}\n`);
      return true;
    }
    case '/models': {
      console.log(`
Available models (run "ollama pull <name>" to download):
  llama3.3        â€” 8B general purpose (recommended)
  qwen2.5:7b      â€” 7B fast and capable
  qwen2.5:3b      â€” 3B very fast, basic tasks
  codestral       â€” specialized for code
  nomic-embed-text â€” embeddings (not for chat)
      `);
      return true;
    }
    case '/history': {
      const turns = messages.filter(m => m.role !== 'system').length;
      const tokens = messages.reduce((s, m) => s + Math.ceil(m.content.split(/\s+/).length * 1.3), 0);
      console.log(`ğŸ“œ ${turns} messages, ~${tokens} tokens\n`);
      return true;
    }
    case '/clear': {
      messages.splice(1);
      console.log('ğŸ—‘ï¸  History cleared\n');
      return true;
    }
    case '/help': {
      console.log(`
Commands:
  /stream           Toggle streaming on/off
  /temp <0-2>       Set temperature
  /model <name>     Switch model
  /models           List available models
  /history          Show conversation size
  /clear            Clear history
  /help             Show this
  exit              Quit
      `);
      return true;
    }
    default: return false;
  }
}

function ask(): void {
  rl.question('You: ', async (input) => {
    if (!input.trim()) { ask(); return; }
    if (input.toLowerCase() === 'exit') { console.log('ğŸ‘‹'); rl.close(); return; }
    if (input.startsWith('/')) { handleCommand(input); ask(); return; }

    try {
      if (useStreaming) {
        await chatWithStreaming(input);
      } else {
        await chatWithoutStreaming(input);
      }
    } catch (err: any) {
      console.error(`\nâŒ ${err.message}\n`);
    }
    ask();
  });
}

console.log('â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—');
console.log('â•‘  ğŸŒ™ Lunar AI â€” v0.1 (streaming mode)  â•‘');
console.log('â•‘  Type /help for commands              â•‘');
console.log('â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n');

ask();
```

### Step 4: Test Streaming vs Non-Streaming (10 minutes)

```
You: Write a short paragraph about TypeScript
Lunar: TypeScript is a... (words appear one by one, like ChatGPT!)

/stream
âœ… Streaming: OFF

You: Write a short paragraph about TypeScript
Lunar is thinking...
Lunar: TypeScript is a typed superset of... (entire response appears at once)

/stream
âœ… Streaming: ON  â† back to streaming, much better UX!
```

### Step 5: Test Model Switching (10 minutes)

```bash
# First, pull a second model (in a separate terminal):
ollama pull qwen2.5:3b
```

```
/model llama3.3
You: Explain closures in JavaScript
Lunar: [detailed, high-quality explanation â€” takes ~5 seconds]

/model qwen2.5:3b
You: Explain closures in JavaScript
Lunar: [shorter, simpler explanation â€” takes ~1 second]
```

**Notice:** The smaller model (3b) is MUCH faster but gives less detailed answers. This is the speed vs quality trade-off in AI.

---

## âœ… CHECKLIST â€” Verify Before Moving to Week 2

- [ ] Streaming works: words appear one by one in the terminal
- [ ] `/stream` toggles between streaming ON/OFF
- [ ] You can switch models with `/model qwen2.5:3b`
- [ ] LLMProvider interface defined cleanly in types.ts
- [ ] OllamaProvider implements both `chat()` and `chatStream()`
- [ ] You can explain: "Streaming is ___ and matters because ___"

---

## ğŸ’¡ KEY TAKEAWAY

**Streaming doesn't make AI faster â€” it makes AI FEEL faster.** Always stream responses when displaying to users. Use a provider interface so you can swap between models/providers without rewriting code. Small models (3b) are fast but basic; large models (8b+) are slower but smarter â€” pick based on the task.

---

## â“ SELF-CHECK QUESTIONS

1. **What JavaScript syntax do you use to read a stream? (hint: you use it for Node.js readable streams too)**
   <details><summary>Answer</summary>`for await (const chunk of stream) { ... }` â€” the async iterator pattern. Same as reading a Node.js ReadableStream.</details>

2. **Should a backend API endpoint that generates a report use streaming? Why or why not?**
   <details><summary>Answer</summary>Usually no. If the backend needs the COMPLETE text before proceeding (to save to database, evaluate, etc.), non-streaming is simpler. Streaming is best when displaying to users in real-time.</details>

3. **What is the benefit of using an LLMProvider interface instead of calling Ollama directly?**
   <details><summary>Answer</summary>You can add new providers (Groq, Gemini, OpenAI) by implementing the same interface, without changing any code that calls `provider.chat()`. It also makes testing easier â€” you can create a mock provider for unit tests.</details>

4. **If llama3.3 (8b) takes 5 seconds and qwen2.5 (3b) takes 1 second for the same question, which should you use?**
   <details><summary>Answer</summary>Depends on the task. For simple Q&A or quick tasks â†’ qwen2.5:3b (faster). For complex reasoning, code, or when quality matters â†’ llama3.3:8b. You might even use both: fast model for simple queries, big model for complex ones.</details>

5. **What does `process.stdout.write()` do differently from `console.log()`?**
   <details><summary>Answer</summary>`process.stdout.write()` prints text WITHOUT adding a newline at the end. `console.log()` always adds a newline. For streaming, we need `write()` so tokens appear on the same line.</details>

---

## ğŸ† WEEK 1 COMPLETE!

```
You now have:
â”œâ”€â”€ âœ… Ollama installed with 2+ models
â”œâ”€â”€ âœ… CLI chatbot with streaming
â”œâ”€â”€ âœ… Multi-turn conversation memory
â”œâ”€â”€ âœ… Configurable temperature and model switching
â”œâ”€â”€ âœ… Clean LLMProvider interface
â”œâ”€â”€ âœ… You understand: tokens, temperature, system prompts, context windows, streaming
â”‚
â”‚  Your project structure:
â”‚  packages/
â”‚  â”œâ”€â”€ agent/src/
â”‚  â”‚   â”œâ”€â”€ llm/
â”‚  â”‚   â”‚   â”œâ”€â”€ client.ts    â† OllamaProvider (streaming + non-streaming)
â”‚  â”‚   â”‚   â””â”€â”€ types.ts     â† Message, LLMProvider, ChatOptions
â”‚  â”‚   â””â”€â”€ cli.ts           â† Interactive REPL with slash commands
â”‚  â””â”€â”€ shared/src/
â”‚      â””â”€â”€ index.ts          â† Version info
â”‚
â””â”€â”€ ğŸ‰ You built your first AI application!
```

**Take the weekend. Rest. You earned it.**

**Next â†’ [Week 2, Day 6: Understanding Tool Calling](../week-02-agent-loop/day-06.md)**
