# Day 1 â€” Environment Setup + What is an LLM?

> ðŸŽ¯ **DAY GOAL:** Install your AI tools and understand what an LLM actually is

---

## ðŸ“š CONCEPT 1: What is an LLM?

### WHAT â€” Simple Definition

**LLM = Large Language Model.** A program that predicts the next word in a sentence.

You type: "The capital of Vietnam is ___"
The LLM predicts: "Hanoi"

That's it. At its core, every ChatGPT, Claude, Gemini â€” they all just predict the next word, over and over, until they finish a response.

### WHY â€” Why Does This Exist?

Before LLMs, if you wanted AI to answer questions, you needed:
- Thousands of hand-written rules ("if user asks X, return Y")
- Separate models for each task (translation, summarization, Q&A)
- Months of engineering per feature

LLMs changed everything: **one model that handles ANY text task.** Write, translate, summarize, code, reason â€” all from the same model. That's why the industry exploded.

### WHEN â€” When Do You Use LLMs?

| Use LLMs For | Don't Use LLMs For |
|---|---|
| Chat interfaces | Simple math (use code) |
| Text summarization | Exact string matching (use regex) |
| Code generation | Database queries (use SQL) |
| Content creation | Real-time data (LLMs have knowledge cutoff) |
| Reasoning over text | Anything requiring 100% accuracy |
| Translation | Passwords, encryption |

### HOW â€” How Does It Work?

Think of it like autocomplete on your phone, but 1000x smarter:

```
Step 1: You send a message
         "What is the capital of Vietnam?"

Step 2: LLM converts your words to numbers (tokens)
         [What][is][the][capital][of][Vietnam][?]
           â†“     â†“    â†“     â†“     â†“     â†“     â†“
         [1024] [72] [89]  [4521] [85] [29834] [30]

Step 3: Numbers go through a neural network (billions of parameters)
         ... math happens here (you don't need to know the math) ...

Step 4: Network outputs probability for next token
         "Hanoi" = 92% probability
         "Ho Chi Minh" = 3%
         "Saigon" = 1%

Step 5: Pick the most likely token â†’ "Hanoi"

Step 6: Repeat from Step 3 until response is complete
         "The" â†’ "capital" â†’ "of" â†’ "Vietnam" â†’ "is" â†’ "Hanoi" â†’ "." â†’ [STOP]
```

**Visual: How an LLM generates text**
```
Your Input         Neural Network            Output (word by word)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ "What is â”‚ â”€â”€â”€â”€ â”‚              â”‚ â”€â”€â”€â”€â”€â”€â–º â”‚ "The"               â”‚
â”‚ the      â”‚      â”‚  Billions of â”‚         â”‚ "The capital"       â”‚
â”‚ capital  â”‚      â”‚  parameters  â”‚         â”‚ "The capital of"    â”‚
â”‚ of       â”‚      â”‚  (weights)   â”‚         â”‚ "The capital of     â”‚
â”‚ Vietnam?"â”‚      â”‚              â”‚         â”‚  Vietnam is Hanoi." â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â†‘
                  Trained on trillions
                  of words from the internet
```

### ðŸ”— NODE.JS ANALOGY

Think of an LLM like a really smart **middleware function**:

```
// In Node.js, you have middleware:
app.use((req, res, next) => {
  // transform request â†’ response
});

// An LLM is similar:
llm.chat((messages) => {
  // input: array of messages (conversation)
  // output: the next message (AI's response)
  // No database, no rules â€” just pattern matching from training
});
```

**The key difference:** Your middleware has explicit logic you wrote. The LLM's logic comes from absorbing patterns in trillions of words. You don't write IF/ELSE â€” the model learned them.

---

## ðŸ“š CONCEPT 2: What is Ollama?

### WHAT â€” Simple Definition

**Ollama = A tool that lets you run LLMs on your own computer.** No cloud, no API key, no cost.

Normally, to use an LLM:
- ChatGPT â†’ need OpenAI API key â†’ costs money per request
- Claude â†’ need Anthropic API key â†’ costs money per request

But with Ollama:
- Download once â†’ runs forever â†’ completely free
- Works offline (airplane mode!)
- Your data never leaves your computer

### WHY â€” Why Use Ollama?

| With Cloud API (ChatGPT) | With Ollama (Local) |
|---|---|
| Costs $0.01-0.06 per request | Free forever |
| Requires internet | Works offline |
| Your data goes to OpenAI servers | Data stays on your Mac |
| Rate limits (429 errors) | No limits |
| API can change or shut down | You control everything |
| Fast (dedicated GPUs) | Slower (uses your CPU/GPU) |

**For learning and development, Ollama is perfect.** No bills, no limits, full control.

### WHEN â€” When to Use What?

```
Learning / Prototyping  â†’ Ollama (free, local)
Need speed in production â†’ Groq (free tier, very fast)
Need best quality        â†’ GPT-4o or Claude (paid)
Need free + good quality â†’ Gemini (free tier, Google)
```

### HOW â€” How Does Ollama Work?

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Your Mac                     â”‚
â”‚                                           â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚ Your Code   â”‚â”€â”€â”€â”€â–ºâ”‚  Ollama      â”‚   â”‚
â”‚   â”‚ (TypeScript)â”‚â—„â”€â”€â”€â”€â”‚  Server      â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚              â”‚   â”‚
â”‚    localhost:          â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚   â”‚
â”‚    any port            â”‚  â”‚ llama3  â”‚ â”‚   â”‚
â”‚                        â”‚  â”‚ (4GB)   â”‚ â”‚   â”‚
â”‚                        â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚   â”‚
â”‚                        â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚   â”‚
â”‚                        â”‚  â”‚ nomic   â”‚ â”‚   â”‚
â”‚                        â”‚  â”‚ (270MB) â”‚ â”‚   â”‚
â”‚                        â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚   â”‚
â”‚                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                         Port 11434        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Your Code calls http://localhost:11434/api/chat
Ollama loads the model into RAM and generates a response
```

### ðŸ”— NODE.JS ANALOGY

Ollama is like **running MongoDB locally** for development:

```
Cloud MongoDB Atlas = Cloud LLM (ChatGPT, Claude)
  â†’ Needs internet, costs money, your data goes elsewhere

Local MongoDB (mongod) = Ollama
  â†’ Free, offline, data stays local, you control everything

// Just like you do:
// mongod --port 27017        â† starts local MongoDB
// ollama serve               â† starts local LLM server

// And connect with:
// mongoose.connect('mongodb://localhost:27017')
// new Ollama({ host: 'http://localhost:11434' })
```

---

## ðŸ“š CONCEPT 3: What is a Monorepo?

### WHAT â€” Simple Definition

**Monorepo = One Git repository with multiple packages inside.** Instead of 5 separate repos, you have 1 repo with 5 folders.

```
lunar/                    â† ONE repo
â”œâ”€â”€ packages/
â”‚   â”œâ”€â”€ agent/            â† Package 1: AI brain
â”‚   â”œâ”€â”€ tools/            â† Package 2: tool functions
â”‚   â”œâ”€â”€ memory/           â† Package 3: RAG + vector search
â”‚   â”œâ”€â”€ connectors/       â† Package 4: Telegram, Discord, etc.
â”‚   â”œâ”€â”€ gateway/          â† Package 5: HTTP server, entry point
â”‚   â””â”€â”€ shared/           â† Package 6: shared types
â””â”€â”€ pnpm-workspace.yaml   â† tells pnpm "these are my packages"
```

### WHY â€” Why Monorepo?

| Separate Repos | Monorepo |
|---|---|
| Each package has its own repo | All packages in one repo |
| Hard to keep in sync | Always in sync |
| Need to publish to npm to share code | Packages import each other directly |
| 5 git repos to manage | 1 git repo to manage |
| Complex CI/CD (5 pipelines) | Simple CI/CD (1 pipeline) |

### WHEN â€” When to Use Monorepo?

- âœ… Multiple packages that depend on each other (like Lunar)
- âœ… Shared types across packages
- âœ… Want atomic commits (change agent + tools in one commit)
- âŒ Completely independent projects that never share code

### HOW â€” How Does pnpm Workspaces Work?

The `pnpm-workspace.yaml` file tells pnpm which folders are packages:

```yaml
# pnpm-workspace.yaml
packages:
  - "packages/*"    # every folder inside packages/ is a package
```

Then each package can import other packages:

```typescript
// In packages/agent/src/runner.ts:
import { executeTool } from '@lunar/tools';  // imports from packages/tools/
import type { Message } from '@lunar/shared';  // imports from packages/shared/
```

---

## ðŸ”¨ HANDS-ON: Setup Environment

### Step 1: Install Ollama (5 minutes)

```bash
# Download from https://ollama.com and install
# After install, verify:
ollama --version
# Should show: ollama version 0.x.x
```

### Step 2: Download AI Models (10-15 minutes, depends on internet)

```bash
# Main chat model â€” this is your AI brain
ollama pull llama3.3
# â†³ Downloads ~4GB, a very capable open-source model

# Embedding model â€” converts text to searchable numbers (we use this Week 3)
ollama pull nomic-embed-text
# â†³ Downloads ~270MB, small and fast
```

### Step 3: Test Your AI (5 minutes)

```bash
# Start a conversation:
ollama run llama3.3 "What is the capital of Vietnam?"
# Expected: "The capital of Vietnam is Hanoi."

# Try a harder question:
ollama run llama3.3 "Explain recursion like I'm 5 years old"
# Expected: A simple, clear explanation

# Try code generation:
ollama run llama3.3 "Write a TypeScript function that reverses a string"
# Expected: A working TypeScript function
```

**ðŸ¤” What happened?** You just talked to an AI running 100% on your computer. No internet needed. No API key. No cost.

### Step 4: Create the Lunar Monorepo (10 minutes)

```bash
# Go to your project folder
cd ~/Documents/project/lunar

# Initialize the root package
pnpm init
# â†³ Creates package.json

# Create workspace config
cat > pnpm-workspace.yaml << 'EOF'
packages:
  - "packages/*"
EOF
# â†³ Tells pnpm: "every folder in packages/ is a separate package"

# Create the package folders
mkdir -p packages/shared/src
mkdir -p packages/agent/src/llm
mkdir -p packages/tools/src
mkdir -p packages/memory/src
mkdir -p packages/connectors/src
mkdir -p packages/session/src
mkdir -p packages/gateway/src

# Verify the structure
find packages -type d | head -20
```

Expected output:
```
packages/shared/src
packages/agent/src/llm
packages/tools/src
packages/memory/src
packages/connectors/src
packages/session/src
packages/gateway/src
```

### Step 5: Configure TypeScript (10 minutes)

```bash
# Install TypeScript tools at the root (workspace-level)
pnpm add -D typescript tsx @types/node -w

# Create TypeScript config
cat > tsconfig.json << 'EOF'
{
  "compilerOptions": {
    "target": "ES2022",
    "module": "Node16",
    "moduleResolution": "Node16",
    "declaration": true,
    "strict": true,
    "esModuleInterop": true,
    "skipLibCheck": true,
    "outDir": "dist",
    "rootDir": ".",
    "baseUrl": ".",
    "paths": {
      "@lunar/*": ["packages/*/src"]
    }
  },
  "include": ["packages/*/src/**/*"]
}
EOF
```

**What each setting means (Node.js developer version):**
| Setting | What It Does | Why |
|---|---|---|
| `target: ES2022` | Output modern JavaScript | We use Node.js 22, it supports ES2022 |
| `module: Node16` | Use Node.js module system | Works with `import/export` + pnpm |
| `strict: true` | Catch more bugs at compile time | Like eslint but for types |
| `paths: @lunar/*` | Import packages by name | `import { x } from '@lunar/shared'` |

### Step 6: Verify Everything Works (5 minutes)

```bash
# Create a test file to verify TypeScript works
cat > packages/shared/src/index.ts << 'EOF'
export const VERSION = '0.1.0';
export const APP_NAME = 'Lunar';
console.log(`${APP_NAME} v${VERSION} â€” Ready!`);
EOF

# Run it with tsx (TypeScript executor â€” no compile step needed)
npx tsx packages/shared/src/index.ts
# Expected output: "Lunar v0.1.0 â€” Ready!"
```

If you see "Lunar v0.1.0 â€” Ready!" â†’ everything works! ðŸŽ‰

---

## âœ… CHECKLIST â€” Verify Before Moving to Day 2

- [ ] `ollama --version` shows a version number
- [ ] `ollama run llama3.3 "Hello"` gives an intelligent response
- [ ] Lunar monorepo structure exists (`packages/agent`, `packages/shared`, etc.)
- [ ] `npx tsx packages/shared/src/index.ts` prints "Lunar v0.1.0 â€” Ready!"
- [ ] You can explain in your own words: "An LLM is ___"

---

## ðŸ’¡ KEY TAKEAWAY

**An LLM is a text-prediction API.** You send text in, you get text out. Ollama lets you run it locally for free. Everything else in this course builds on top of this one simple idea.

---

## â“ SELF-CHECK QUESTIONS

Test yourself. Answer without looking up:

1. **What does LLM stand for?**
   <details><summary>Answer</summary>Large Language Model</details>

2. **How does an LLM generate a response? (1 sentence)**
   <details><summary>Answer</summary>It predicts the next word/token over and over until the response is complete.</details>

3. **Why use Ollama instead of ChatGPT API for learning?**
   <details><summary>Answer</summary>Free, no API key needed, works offline, data stays on your computer, no rate limits.</details>

4. **What is a monorepo?**
   <details><summary>Answer</summary>One Git repository containing multiple packages that can import each other.</details>

5. **What file tells pnpm which folders are packages?**
   <details><summary>Answer</summary>pnpm-workspace.yaml</details>

6. **If you ask an LLM "What's the weather right now?", will it give an accurate answer? Why or why not?**
   <details><summary>Answer</summary>No. LLMs only predict text based on training data â€” they can't access real-time information. They have a knowledge cutoff date and cannot check the actual weather.</details>

---

**Next â†’ [Day 2: First LLM Call from TypeScript](day-02.md)**
