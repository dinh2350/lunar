# Day 25 â€” CI Eval Pipeline + Quality Gates

> ğŸ¯ **DAY GOAL:** Automate evaluation so every code change gets tested â€” like CI/CD but for AI quality

---

## ğŸ“š CONCEPT 1: CI for AI â€” Why It's Different

### WHAT â€” Simple Definition

**Continuous Integration (CI) for AI adds evaluation scoring on top of normal tests.** Instead of just "does it compile?", you also check "is the AI still good?"

```
TRADITIONAL CI:                    AI CI:
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ git push â”‚                     â”‚ git push â”‚
  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜                     â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
       â”‚                                â”‚
  â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”                     â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”
  â”‚ lint     â”‚                     â”‚ lint     â”‚
  â”‚ typecheckâ”‚                     â”‚ typecheckâ”‚
  â”‚ unit testâ”‚                     â”‚ unit testâ”‚
  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜                     â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
       â”‚                                â”‚
       âœ… Done                     â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
                                   â”‚ EVAL RUN  â”‚  â† NEW
                                   â”‚ 6 tests   â”‚
                                   â”‚ score>0.7 â”‚
                                   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                                        â”‚
                                   â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
                                   â”‚ QUALITY   â”‚  â† NEW
                                   â”‚ GATE      â”‚
                                   â”‚ pass/fail â”‚
                                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### WHY â€” Why Automate Evals?

```
Manual testing:
  âŒ "I'll just try a few questions manually"
  âŒ Forget to test after every change
  âŒ Can't compare results consistently
  âŒ No proof of quality for job interviews

Automated eval CI:
  âœ… Runs on every push automatically
  âœ… Blocks merges if quality drops
  âœ… Historical quality tracking
  âœ… "Show me your CI pipeline" â†’ impressive in interviews
```

### ğŸ”— NODE.JS ANALOGY

```
Quality Gate = test coverage threshold

// jest.config.js
coverageThreshold: {
  global: { branches: 80, functions: 80 }
}
// CI fails if coverage drops below 80%

// eval_config
qualityThreshold: {
  overall: 0.7,
  rag: 0.75,
  safety: 0.9
}
// CI fails if eval score drops below threshold
```

---

## ğŸ“š CONCEPT 2: GitHub Actions for AI Projects

### WHAT â€” Simple Definition

**A workflow file that runs your eval suite on every push, and blocks the PR if scores drop below thresholds.**

### HOW â€” The Pipeline Steps

```
Trigger: push to main or PR
  â”‚
  â”œâ”€â”€ Job 1: Standard checks (parallel)
  â”‚   â”œâ”€â”€ pnpm lint
  â”‚   â”œâ”€â”€ pnpm typecheck
  â”‚   â””â”€â”€ pnpm test
  â”‚
  â””â”€â”€ Job 2: AI Eval (after Job 1 passes)
      â”œâ”€â”€ Start Ollama
      â”œâ”€â”€ Pull model (qwen2.5:7b)
      â”œâ”€â”€ Start Lunar gateway
      â”œâ”€â”€ Start eval service
      â”œâ”€â”€ Run eval dataset
      â”œâ”€â”€ Check quality gate
      â””â”€â”€ Upload report as artifact
```

---

## ğŸ”¨ HANDS-ON: Build the CI Pipeline

### Step 1: Quality Gate Script (20 minutes)

Create `services/eval/gate.py`:

```python
"""Quality gate â€” checks if eval results meet thresholds."""
import json
import sys
from pathlib import Path

# Thresholds per category
THRESHOLDS = {
    "overall": 0.70,
    "knowledge": 0.70,
    "rag": 0.75,
    "reasoning": 0.60,
    "safety": 0.90,
}

def check_gate(report_path: str) -> bool:
    """Check if eval results pass quality gate."""
    with open(report_path) as f:
        results = json.load(f)
    
    if not results:
        print("âŒ No results found!")
        return False
    
    # Overall score
    overall = sum(r["overall_score"] for r in results) / len(results)
    print(f"\nğŸ“Š Quality Gate Check")
    print(f"{'='*50}")
    
    passed = True
    
    # Check overall
    status = "âœ…" if overall >= THRESHOLDS["overall"] else "âŒ"
    if overall < THRESHOLDS["overall"]:
        passed = False
    print(f"  {status} Overall: {overall:.3f} (min: {THRESHOLDS['overall']})")
    
    # Check per category
    categories = set(r["category"] for r in results)
    for cat in sorted(categories):
        cat_results = [r for r in results if r["category"] == cat]
        cat_score = sum(r["overall_score"] for r in cat_results) / len(cat_results)
        threshold = THRESHOLDS.get(cat, THRESHOLDS["overall"])
        status = "âœ…" if cat_score >= threshold else "âŒ"
        if cat_score < threshold:
            passed = False
        print(f"  {status} {cat:15s}: {cat_score:.3f} (min: {threshold})")
    
    print(f"{'='*50}")
    
    if passed:
        print("âœ… QUALITY GATE PASSED\n")
    else:
        print("âŒ QUALITY GATE FAILED\n")
    
    return passed


if __name__ == "__main__":
    if len(sys.argv) < 2:
        # Find most recent report
        reports = sorted(Path("reports").glob("eval_*.json"))
        if not reports:
            print("No reports found! Run eval first.")
            sys.exit(1)
        report_path = str(reports[-1])
    else:
        report_path = sys.argv[1]
    
    print(f"Checking: {report_path}")
    ok = check_gate(report_path)
    sys.exit(0 if ok else 1)
```

### Step 2: GitHub Actions Workflow (20 minutes)

Create `.github/workflows/eval.yml`:

```yaml
name: AI Eval Pipeline

on:
  push:
    branches: [main]
  pull_request:
    branches: [main]

jobs:
  # Job 1: Standard checks
  checks:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: pnpm/action-setup@v4
      - uses: actions/setup-node@v4
        with:
          node-version: 22
          cache: pnpm
      - run: pnpm install
      - run: pnpm lint
      - run: pnpm typecheck
      - run: pnpm test

  # Job 2: AI Evaluation
  eval:
    runs-on: ubuntu-latest
    needs: checks  # only run if checks pass
    steps:
      - uses: actions/checkout@v4

      # Node.js setup
      - uses: pnpm/action-setup@v4
      - uses: actions/setup-node@v4
        with:
          node-version: 22
          cache: pnpm

      # Python setup
      - uses: actions/setup-python@v5
        with:
          python-version: "3.12"
      
      # Install Ollama
      - name: Install Ollama
        run: |
          curl -fsSL https://ollama.com/install.sh | sh
          ollama serve &
          sleep 5
          ollama pull qwen2.5:7b

      # Install dependencies
      - name: Install Node dependencies
        run: pnpm install

      - name: Install Python dependencies
        run: |
          cd services/eval
          python -m venv .venv
          source .venv/bin/activate
          pip install -r requirements.txt

      # Start services
      - name: Start services
        run: |
          # Start Lunar gateway
          pnpm dev &
          sleep 10
          
          # Start eval service
          cd services/eval
          source .venv/bin/activate
          uvicorn main:app --port 8000 &
          sleep 5

      # Run evaluation
      - name: Run eval suite
        run: |
          cd services/eval
          source .venv/bin/activate
          python runner.py

      # Quality gate
      - name: Check quality gate
        run: |
          cd services/eval
          source .venv/bin/activate
          python gate.py

      # Save report
      - name: Upload eval report
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: eval-report
          path: services/eval/reports/
```

### Step 3: Local CI Script (10 minutes)

Create `scripts/eval.sh`:

```bash
#!/bin/bash
# Run eval pipeline locally â€” same as CI but on your machine
set -e

echo "ğŸŒ™ Lunar Eval Pipeline"
echo "======================"

# Check prerequisites
echo "Checking prerequisites..."
command -v ollama >/dev/null 2>&1 || { echo "âŒ Ollama not found"; exit 1; }
command -v python3 >/dev/null 2>&1 || { echo "âŒ Python not found"; exit 1; }

# Start services (if not already running)
echo "Starting services..."

# Check if Lunar is running
if ! curl -s http://localhost:3100/api/health > /dev/null 2>&1; then
    echo "  Starting Lunar gateway..."
    pnpm dev &
    LUNAR_PID=$!
    sleep 10
fi

# Check if eval service is running
if ! curl -s http://localhost:8000/health > /dev/null 2>&1; then
    echo "  Starting eval service..."
    cd services/eval
    source .venv/bin/activate
    uvicorn main:app --port 8000 &
    EVAL_PID=$!
    cd ../..
    sleep 5
fi

# Run eval
echo "Running evaluation..."
cd services/eval
source .venv/bin/activate
python runner.py

# Quality gate
echo "Checking quality gate..."
python gate.py
EXIT_CODE=$?

# Cleanup background processes
[ -n "$LUNAR_PID" ] && kill $LUNAR_PID 2>/dev/null
[ -n "$EVAL_PID" ] && kill $EVAL_PID 2>/dev/null

exit $EXIT_CODE
```

```bash
chmod +x scripts/eval.sh
```

### Step 4: Add to package.json (5 minutes)

```json
{
  "scripts": {
    "dev": "pnpm --filter @lunar/gateway dev",
    "eval": "./scripts/eval.sh",
    "eval:quick": "cd services/eval && source .venv/bin/activate && python runner.py"
  }
}
```

---

## ğŸ“š CONCEPT 3: Tracking Quality Over Time

### WHAT â€” Simple Definition

**Save every eval report with a timestamp. Compare reports to see if quality improves or degrades over time.**

```
reports/
â”œâ”€â”€ eval_20260225_143000.json   â† score: 0.78
â”œâ”€â”€ eval_20260226_093000.json   â† score: 0.82  â†‘ improved!
â”œâ”€â”€ eval_20260227_161500.json   â† score: 0.71  â†“ regression!
â””â”€â”€ eval_20260228_110000.json   â† score: 0.85  â†‘ fixed!
```

### WHY â€” What This Proves in Interviews

```
Interviewer: "How do you ensure AI quality?"
You: "I built an automated eval pipeline:
  1. Eval dataset with 20+ test cases across 5 categories
  2. LLM-as-Judge scoring with rubrics
  3. Quality gates â€” PR blocked if score < 0.7
  4. Trend tracking â€” I can show quality improving over time
  5. Runs in GitHub Actions on every push"

This answer demonstrates:
  âœ… Production AI engineering practices
  âœ… Testing methodology
  âœ… CI/CD experience
  âœ… Quality-first mindset
```

---

## âœ… CHECKLIST

- [ ] Quality gate script checks thresholds per category
- [ ] GitHub Actions workflow file created
- [ ] Local eval script (`scripts/eval.sh`) works
- [ ] Reports saved with timestamps
- [ ] Quality gate exits with code 0 (pass) or 1 (fail)
- [ ] Can run `pnpm eval` to trigger full pipeline

---

## ğŸ’¡ KEY TAKEAWAY

**Automated AI evaluation is your most impressive engineering asset. It's CI/CD for intelligence â€” every push gets tested not just for bugs but for quality. Quality gates prevent regressions. This is what separates hobby projects from production AI systems.**

---

## ğŸ† WEEK 5 COMPLETE!

**What you built this week:**
- âœ… Python basics (enough to read AI code)
- âœ… FastAPI eval microservice
- âœ… LLM-as-Judge evaluation
- âœ… Automated eval dataset + runner
- âœ… CI pipeline with quality gates

**Next â†’ [Day 26: Docker Fundamentals](../../phase-2-production/week-06-docker/day-26.md)**
