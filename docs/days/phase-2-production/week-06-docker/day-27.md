# Day 27 ‚Äî Docker Compose: Multi-Service Setup

> üéØ **DAY GOAL:** Run Lunar + Ollama + Eval Service together with one command using Docker Compose

---

## üìö CONCEPT 1: What is Docker Compose?

### WHAT ‚Äî Simple Definition

**Docker Compose runs multiple containers together as one system.** One YAML file describes all your services, networks, and volumes.

```
WITHOUT COMPOSE (manual):
  Terminal 1: docker run ollama/ollama
  Terminal 2: docker run lunar
  Terminal 3: docker run lunar-eval
  Link them together manually... üò©

WITH COMPOSE (one command):
  docker compose up
  ‚Üí Starts all 3 services
  ‚Üí Creates network between them
  ‚Üí Manages volumes for data
  ‚Üí One Ctrl+C stops everything
```

### WHY ‚Äî Why Compose for AI Projects?

```
Lunar needs multiple services:
  1. Ollama (LLM server)           ‚Äî port 11434
  2. Lunar Gateway (your agent)    ‚Äî port 3100
  3. Eval Service (Python)         ‚Äî port 8000

These need to:
  ‚Üí Talk to each other (Lunar ‚Üí Ollama, Lunar ‚Üí Eval)
  ‚Üí Start in the right order (Ollama first!)
  ‚Üí Share data (volumes for models, memory)
  ‚Üí Have consistent configuration
```

### üîó NODE.JS ANALOGY

```
Docker Compose = package.json scripts + concurrently

// package.json (without compose)
"scripts": {
  "dev:ollama": "ollama serve",
  "dev:gateway": "tsx watch packages/gateway/src/index.ts",
  "dev:eval": "cd services/eval && uvicorn main:app",
  "dev": "concurrently pnpm:dev:*"    // run all
}

// docker-compose.yml (with compose)
services:
  ollama:  ...  // same as dev:ollama
  gateway: ...  // same as dev:gateway
  eval:    ...  // same as dev:eval
```

---

## üìö CONCEPT 2: Compose File Anatomy

### HOW ‚Äî Section by Section

```yaml
# version is implicit in modern Compose

services:          # ‚Üê Define each container
  ollama:          # ‚Üê Service name (becomes hostname)
    image: ...     # ‚Üê Which image to use
    ports: ...     # ‚Üê Expose ports to host
    volumes: ...   # ‚Üê Persistent storage

  gateway:
    build: ...     # ‚Üê Build from Dockerfile
    depends_on: .. # ‚Üê Start order
    environment: . # ‚Üê Environment variables

volumes:           # ‚Üê Named volumes for data
  ollama-data:
  lunar-data:

networks:          # ‚Üê Usually auto-created (default)
```

### Key Concepts

```
SERVICE NAMES = HOSTNAMES
  Within compose network:
    ollama  ‚Üí http://ollama:11434
    gateway ‚Üí http://gateway:3100
    eval    ‚Üí http://eval:8000

  From your laptop:
    ‚Üí http://localhost:11434
    ‚Üí http://localhost:3100
    ‚Üí http://localhost:8000

DEPENDS_ON = START ORDER
  gateway depends on ollama
  ‚Üí Ollama starts first, gateway waits

VOLUMES = PERSISTENT STORAGE
  Without volume: data lost when container stops
  With volume: data persists between restarts
```

---

## üî® HANDS-ON: Create Docker Compose for Lunar

### Step 1: Create docker-compose.yml (30 minutes)

Create `docker-compose.yml` in project root:

```yaml
# ============================================
# Lunar AI Agent ‚Äî Docker Compose
# ============================================
# Start everything: docker compose up
# Stop everything:  docker compose down
# Rebuild:          docker compose up --build

services:
  # ---- 1. Ollama (LLM Server) ----
  ollama:
    image: ollama/ollama:latest
    container_name: lunar-ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama-data:/root/.ollama    # persist downloaded models
    deploy:
      resources:
        reservations:
          memory: 4G    # LLMs need RAM
    healthcheck:
      test: ["CMD", "ollama", "list"]
      interval: 30s
      timeout: 10s
      retries: 3

  # ---- 2. Lunar Gateway (Main Agent) ----
  gateway:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: lunar-gateway
    ports:
      - "3100:3100"
    environment:
      - LUNAR_PORT=3100
      - LUNAR_AGENT=main
      - LUNAR_MODEL=qwen2.5:3b
      - OLLAMA_URL=http://ollama:11434    # ‚Üê service name!
      - EVAL_URL=http://eval:8000         # ‚Üê service name!
      - NODE_ENV=production
    volumes:
      - lunar-data:/home/lunar/.lunar     # persist memory + sessions
    depends_on:
      ollama:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3100/api/health"]
      interval: 15s
      timeout: 5s
      retries: 3

  # ---- 3. Eval Service (Python) ----
  eval:
    build:
      context: ./services/eval
      dockerfile: Dockerfile
    container_name: lunar-eval
    ports:
      - "8000:8000"
    environment:
      - OLLAMA_URL=http://ollama:11434
    depends_on:
      ollama:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 15s
      timeout: 5s
      retries: 3

  # ---- 4. Model Puller (init container ‚Äî runs once) ----
  model-puller:
    image: curlimages/curl:latest
    container_name: lunar-model-puller
    depends_on:
      ollama:
        condition: service_healthy
    entrypoint: >
      sh -c "
        echo 'Pulling models...' &&
        curl -s http://ollama:11434/api/pull -d '{\"name\":\"qwen2.5:3b\"}' &&
        curl -s http://ollama:11434/api/pull -d '{\"name\":\"nomic-embed-text\"}' &&
        echo 'Models ready!'
      "
    restart: "no"

volumes:
  ollama-data:
    name: lunar-ollama-data
  lunar-data:
    name: lunar-data
```

### Step 2: Eval Service Dockerfile (10 minutes)

Create `services/eval/Dockerfile`:

```dockerfile
FROM python:3.12-slim

WORKDIR /app

# Install dependencies
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copy code
COPY . .

# Non-root user
RUN useradd --create-home eval
USER eval

EXPOSE 8000

CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000"]
```

### Step 3: Run Everything (10 minutes)

```bash
# Start all services
docker compose up -d
# [+] Running 4/4
# ‚úî ollama      Started
# ‚úî model-puller Started
# ‚úî eval        Started
# ‚úî gateway     Started

# Watch logs
docker compose logs -f
# lunar-ollama   | starting Ollama server...
# lunar-model-puller | Pulling models...
# lunar-eval     | INFO: Uvicorn running on 0.0.0.0:8000
# lunar-gateway  | üåô Lunar Gateway starting...
# lunar-gateway  | üåô Lunar is ready!

# Check all services
docker compose ps
# NAME               STATUS      PORTS
# lunar-ollama       running     11434
# lunar-gateway      running     3100
# lunar-eval         running     8000
# lunar-model-puller exited(0)

# Test
curl http://localhost:3100/api/health
curl http://localhost:8000/health
```

### Step 4: Useful Compose Commands (10 minutes)

```bash
# Start all
docker compose up -d              # detached (background)
docker compose up                 # foreground (see all logs)

# Stop all
docker compose down               # stop and remove containers
docker compose down -v            # also remove volumes (‚ö†Ô∏è data loss!)

# Rebuild after code changes
docker compose up --build -d      # rebuild changed images

# Logs
docker compose logs gateway       # one service
docker compose logs -f            # follow all
docker compose logs --tail=50     # last 50 lines

# Shell into a container
docker compose exec gateway /bin/sh
docker compose exec ollama /bin/sh

# Restart one service
docker compose restart gateway

# Scale (future: multiple agent instances)
docker compose up -d --scale gateway=2

# Resource usage
docker compose stats
```

---

## ‚úÖ CHECKLIST

- [ ] docker-compose.yml with 4 services created
- [ ] Eval service Dockerfile created
- [ ] `docker compose up` starts everything
- [ ] Services can talk to each other by name
- [ ] Ollama models persist in volume
- [ ] Lunar data persists in volume
- [ ] Health checks working for all services

---

## üí° KEY TAKEAWAY

**Docker Compose turns "install this, configure that, start these 3 terminals" into `docker compose up`. Service names become hostnames (ollama ‚Üí http://ollama:11434). Volumes persist data. Health checks ensure services start in order. One file defines your entire AI system.**

---

**Next ‚Üí [Day 28: Docker Volumes, Networks, and Security](day-28.md)**
